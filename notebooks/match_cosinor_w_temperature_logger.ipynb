{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e0bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from definitions import RAW_DATA_DIR, INTERIM_DATA_DIR\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5942ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean IDs and join tables\n",
    "\n",
    "def _format_cosinor_id(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Formats cosinor ID strings to match format of temp logger IDs in masterfile.\n",
    "    exmples:\n",
    "    - \"M0122\" -> 122\n",
    "    - \"M1058\" -> 1058\n",
    "\n",
    "    args:\n",
    "        series (pd.Series): Series of cosinor ID strings.\n",
    "    \"\"\"\n",
    "    cleaned = (\n",
    "        series.astype(str)\n",
    "        .str.replace(r\"\\D+\", \"\", regex=True)  # drop non-digits\n",
    "        .str.lstrip(\"0\")\n",
    "        .replace(\"\", np.nan)\n",
    "    )\n",
    "    return pd.to_numeric(cleaned, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "\n",
    "def _format_temp_logger_id(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Formats temp logger ID strings to be a string representation of an integer.\n",
    "    examples:\n",
    "    - \"122.0 \" -> 122\n",
    "\n",
    "    args:\n",
    "        series (pd.Series): Series of temp logger ID strings.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned = (\n",
    "        series.astype(str)\n",
    "        .str.strip()\n",
    "        .str.split(\".\", n=1)\n",
    "        .str[0]\n",
    "        .str.replace(r\"\\D+\", \"\", regex=True)\n",
    "        .str.lstrip(\"0\")\n",
    "        .replace(\"\", np.nan)\n",
    "    )\n",
    "    return pd.to_numeric(cleaned, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def validate_column(df_to_validate, column_name, pattern_re_string, accept_nulls=True, ignore_duplicates=True):\n",
    "    print(\"Original df shape:\", df_to_validate.shape)\n",
    "\n",
    "    return_df = df_to_validate.copy()\n",
    "\n",
    "    if not ignore_duplicates:\n",
    "        duplicates_df = check_for_duplicates(df_to_validate, column_name, ignore_na=accept_nulls)\n",
    "        if not duplicates_df.empty:\n",
    "            print(f\"\\nRemoving {len(duplicates_df)} duplicate rows\")\n",
    "            return_df = return_df.drop(duplicates_df.index)\n",
    "            print(f\"New df shape after removing duplicates: {return_df.shape}\")\n",
    "\n",
    "    if not accept_nulls:\n",
    "        # Check for null values\n",
    "        null_mask = df_to_validate[column_name].isna()\n",
    "    else:\n",
    "        null_mask = pd.Series([False] * len(df_to_validate), index=df_to_validate.index)\n",
    "        \n",
    "    # Check for invalid patterns\n",
    "    valid_pattern = re.compile(pattern=pattern_re_string)\n",
    "    pattern_mask = ~df_to_validate[column_name].astype(str).str.match(valid_pattern, na=False)\n",
    "\n",
    "    # Combine masks\n",
    "    invalid_mask = null_mask | pattern_mask\n",
    "\n",
    "    if invalid_mask.sum() > 0:\n",
    "        print(f\"\\nRows with null value: {null_mask.sum()}\")\n",
    "        print(f\"Rows with invalid pattern: {pattern_mask.sum()}\")\n",
    "        print(f\"Total invalid rows: {invalid_mask.sum()}\")\n",
    "        print(f\"\\nInvalid values in '{column_name}':\")\n",
    "        print(df_to_validate.loc[invalid_mask, column_name])\n",
    "        \n",
    "        print(f\"\\nRemoving {invalid_mask.sum()} rows with invalid values\")\n",
    "        return_df = df_to_validate[~invalid_mask].copy()\n",
    "        print(f\"New df shape: {return_df.shape}\")\n",
    "    else:\n",
    "        print(\"No invalid IDs found.\")\n",
    "    \n",
    "    return return_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ee6d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_for_duplicates(df_to_check, column_name, ignore_na=True):\n",
    "    if ignore_na:\n",
    "        # Get the non-null values and find duplicates\n",
    "        non_null_values = df_to_check[column_name].dropna()\n",
    "        duplicated_values = non_null_values[non_null_values.duplicated(keep=False)]\n",
    "        # Create mask for all rows with duplicated values\n",
    "        duplicated_mask = df_to_check[column_name].isin(duplicated_values)\n",
    "    else:\n",
    "        duplicated_mask = df_to_check[column_name].duplicated(keep=False)\n",
    "    \n",
    "    if duplicated_mask.sum() > 0:\n",
    "        print(f\"\\nDuplicate entries found in column '{column_name}':\")\n",
    "        print(df_to_check.loc[duplicated_mask, column_name])\n",
    "    else:\n",
    "        print(f\"No duplicates found in column '{column_name}'.\")\n",
    "\n",
    "    # return duplicated values\n",
    "    return df_to_check[duplicated_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "842fd500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df shape: (476, 2)\n",
      "No invalid IDs found.\n"
     ]
    }
   ],
   "source": [
    "# Load the prefixes scraped from cosinor analysis files\n",
    "cosinor_ids = pd.read_parquet(INTERIM_DATA_DIR / \"cosinor_IDs.parquet\")\n",
    "# Validate cosinor_ids\n",
    "cosinor_ids = validate_column(cosinor_ids, \"cosinor_id\", r'^[M0-9]+$', accept_nulls=False)\n",
    "cosinor_ids[\"cosinor_id_formatted\"] = _format_cosinor_id(cosinor_ids[\"cosinor_id\"])\n",
    "temp_columns.append(\"cosinor_id_formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69e6fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile = pd.read_parquet(RAW_DATA_DIR / \"Heat Stress Masterfile May 2024 - RF Ewe.ram data.parquet\")\n",
    "# Track temporary columns we'll add so we can remove them later\n",
    "temp_columns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bf8e3",
   "metadata": {},
   "source": [
    "### Temp logger 2023\n",
    "join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "874971b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate entries found in column 'temp_logger_2023_formatted':\n",
      "274    718\n",
      "901    718\n",
      "Name: temp_logger_2023_formatted, dtype: Int64\n",
      "The dupes don't have a cosinor analysis, so we don't care about them\n"
     ]
    }
   ],
   "source": [
    "masterfile[\"temp_logger_2023_formatted\"] = _format_temp_logger_id(masterfile[\"Temp logger # 2023\"])\n",
    "temp_columns.append(\"temp_logger_2023_formatted\")\n",
    "# Check for duplicates in temp_logger_2023 column\n",
    "duplicated_df = check_for_duplicates(masterfile, \"temp_logger_2023_formatted\")\n",
    "duplicate_logger_ids = duplicated_df[\"temp_logger_2023_formatted\"].to_list()\n",
    "\n",
    "if duplicate_logger_ids:\n",
    "    # are any of the duplicates also cosinor_ids?\n",
    "    overlap = [d for d in duplicate_logger_ids if d in cosinor_ids[\"cosinor_id_formatted\"].to_list()]\n",
    "    if not overlap:\n",
    "        print(\"The dupes don't have a cosinor analysis, so we don't care about them\")\n",
    "    else:\n",
    "        print(f\"Dupes {overlap} have matching cosinor analysis. This is a PROBLEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ea9f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_2023 = cosinor_ids.merge(\n",
    "    masterfile,\n",
    "    how=\"left\",\n",
    "    left_on=\"cosinor_id_formatted\",\n",
    "    right_on=\"temp_logger_2023_formatted\",\n",
    "    suffixes=(\"\", \"_masterfile\"),\n",
    "    indicator=True, # adds a column called “_merge” with the source of each row\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8391830c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined table shape: (476, 123)\n",
      "Rows with match: 460\n",
      "Rows without match: 16\n",
      "\n",
      "First 5 rows:\n",
      "  cosinor_id  cosinor_id_formatted  Temp logger # 2023 _merge\n",
      "0      M0122                   122                 122   both\n",
      "1      M0123                   123                 123   both\n",
      "2      M0124                   124                 124   both\n",
      "3      M0125                   125                 125   both\n",
      "4      M0126                   126                 126   both\n"
     ]
    }
   ],
   "source": [
    "print(f\"Joined table shape: {joined_2023.shape}\")\n",
    "print(f\"Rows with match: {(joined_2023['_merge'] == 'both').sum()}\")\n",
    "print(f\"Rows without match: {(joined_2023['_merge'] == 'left_only').sum()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(joined_2023[['cosinor_id', 'cosinor_id_formatted', 'Temp logger # 2023', '_merge']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24fbb2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('E:/alexa/No-OneDrive/Code/proj/disco-baa-01/data/02_interim/cosinor_IDs_masterfile_2023.parquet')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save join of 2023 starters\n",
    "joined_2023 = joined_2023.assign(starter_year=2023)\n",
    "out_2023 = INTERIM_DATA_DIR / f\"{JOIN_NAME}_2023.parquet\"\n",
    "joined_2023.to_parquet(out_2023, index=False)\n",
    "out_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc7c0d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('E:/alexa/No-OneDrive/Code/proj/disco-baa-01/data/02_interim/cosinor_IDs_masterfile_2024.parquet')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat for 2024 starters\n",
    "if \"Temp logger # 2024\" not in masterfile.columns:\n",
    "    raise KeyError(\"Temp logger # 2024 column not found in masterfile\")\n",
    "\n",
    "masterfile[\"temp_logger_2024_formatted\"] = _format_temp_logger_id(masterfile[\"Temp logger # 2024\"])\n",
    "\n",
    "joined_2024 = cosinor_ids.merge(\n",
    "    masterfile,\n",
    "    how=\"left\",\n",
    "    left_on=\"cosinor_id_formatted\",\n",
    "    right_on=\"temp_logger_2024_formatted\",\n",
    "    suffixes=(\"\", \"_masterfile\"),\n",
    "    indicator=True,\n",
    ")\n",
    "\n",
    "joined_2024 = joined_2024.assign(starter_year=2024)\n",
    "out_2024 = INTERIM_DATA_DIR / f\"{JOIN_NAME}_2024.parquet\"\n",
    "joined_2024.to_parquet(out_2024, index=False)\n",
    "out_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c856250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with both temp logger values: 0\n",
      "\n",
      "No rows have values for both temp logger columns.\n"
     ]
    }
   ],
   "source": [
    "# Check if any row has non-empty values for both temp logger columns\n",
    "both_cols_2023 = masterfile[\"Temp logger # 2023\"].notna()\n",
    "both_cols_2024 = masterfile[\"Temp logger # 2024\"].notna()\n",
    "\n",
    "both_have_values = masterfile[both_cols_2023 & both_cols_2024]\n",
    "\n",
    "print(f\"Number of rows with both temp logger values: {len(both_have_values)}\")\n",
    "if len(both_have_values) > 0:\n",
    "    print(\"\\nRows with both values:\")\n",
    "    print(both_have_values[[\"Temp logger # 2023\", \"Temp logger # 2024\"]].head(20))\n",
    "else:\n",
    "    print(\"\\nNo rows have values for both temp logger columns.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
